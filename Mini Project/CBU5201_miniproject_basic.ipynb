{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGn4ICrfqXZ"
      },
      "source": [
        "# 1 Author\n",
        "\n",
        "**Student Name**:  Liang Zheyu 梁哲与   \n",
        "**Student ID**:  210977800\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38VQkcdKd6k"
      },
      "source": [
        "# 2 Problem formulation\n",
        "\n",
        "Describe the machine learning problem that you want to solve and explain what's interesting about it.   \n",
        "-   \n",
        "Using the genki4k dataset, build a machine learning pipeline that takes as an input an image and predicts 1) whether the person in the image is similing or not 2) estimate the 3D head pose labels in the image.   \n",
        "The interesting part of this problem is that it is a multi-task problem. First it needs to predict whether the people is smiling and also estimate the 3D head pose labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BwrtEdLDit"
      },
      "source": [
        "# 3 Machine Learning pipeline\n",
        "\n",
        "Describe your ML pipeline. Clearly identify its input and output, any intermediate stages (for instance, transformation -> models), and intermediate data moving from one stage to the next. It's up to you to decide which stages to include in your pipeline.   \n",
        "-   \n",
        "The input of the pipeline is the image data. The output of the pipeline is the prediction of the smile and the 3D head pose labels. The intermediate stages are the transformation and the models. The transformation stage is to transform the image data into the format that the model can use. The model stage is to train the model and predict the result.   \n",
        "   \n",
        "Preprocess dataset => Train the model => Evaluate the model.   \n",
        "   \n",
        "**Classification:**   \n",
        "The input of the classification model is the image data. The output of the classification model is the prediction of the smile. Make it into 0-1.   \n",
        "   \n",
        "**Regression:**   \n",
        "The input of the regression model is the image data. The output of the regression model is the prediction of the 3D head pose labels. Including the yaw, pitch and roll. All are float numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import os\n",
        "from PIL import Image\n",
        "import datetime\n",
        "\n",
        "\n",
        "# 参数-------------------------------------\n",
        "epochs = 10\n",
        "isSaveModel = False\n",
        "# 参数-------------------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nDXnzYLLH6"
      },
      "source": [
        "# 4 Transformation stage\n",
        "\n",
        "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage.   \n",
        "-   \n",
        "The transformation stage is to transform the image data into the format that the model can use. Load the image data and labels.   \n",
        "   \n",
        "Consistency: Transforming all images into the same RGB format and size ensures a consistent input format.   \n",
        "   \n",
        "Data Augmentation: Introducing random horizontal flipping during training enhances data diversity, aiding the model in better generalizing to various samples.   \n",
        "   \n",
        "Normalization: Standardizing the input accelerates the training process and assists the model in handling pixel values from different ranges.   \n",
        "   \n",
        "The selection of this transformation stage aims to ensure the consistency, diversity, and stability of input data, thereby improving the model's performance.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F5_kI95LuZ2"
      },
      "source": [
        "# 5 Modelling\n",
        "\n",
        "Describe the ML model(s) that you will build. Explain why you have chosen them.   \n",
        "-   \n",
        "This code defines a class named AlexNet, which inherits from the nn.Module module in PyTorch.\n",
        "\n",
        "In the constructor of this class, the pre-trained AlexNet model is loaded and assigned to self.alexnet. Then, by iterating through all the parameters of the model and setting their requires_grad attribute to False, all layers of the pre-trained model are frozen. This means that during training, the weights of these layers will not be updated.\n",
        "\n",
        "Next, it gets the number of input features of the last layer (a fully connected layer) of the original AlexNet model's classifier and replaces it with a new fully connected layer. This new fully connected layer has the same number of input features as the original one, but the number of output features is set to 2, meaning that this model is customized as a binary classification model.\n",
        "\n",
        "In the forward method, the input data x is passed to self.alexnet for forward propagation and the output result is returned.\n",
        "\n",
        "The reason for choosing AlexNet is that its pre-trained model has been trained on a large amount of image data and can extract effective features. Moreover, by freezing the layers of the pre-trained model and replacing the fully connected layer, it can be easily customized as a model for a specific task.And by comparing to other models, AlexNet is trainning faster and has a better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlexNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet2, self).__init__()\n",
        "        self.alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "        # 冻结预训练模型的所有层\n",
        "        for param in self.alexnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        num_ftrs = self.alexnet.classifier[6].in_features\n",
        "        self.alexnet.classifier[6] = nn.Linear(num_ftrs, 2)  # 替换全连接层\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alexnet(x)\n",
        "        return x\n",
        "\n",
        "class AlexNet3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet3, self).__init__()\n",
        "        self.alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "        # 冻结预训练模型的所有层\n",
        "        for param in self.alexnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        num_ftrs = self.alexnet.classifier[6].in_features\n",
        "        self.alexnet.classifier[6] = nn.Linear(num_ftrs, 3)  # 替换全连接层\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alexnet(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPTSuaB9L2jU"
      },
      "source": [
        "# 6 Methodology\n",
        "\n",
        "Describe how you will train and validate your models, how model performance is assesssed (i.e. accuracy, confusion matrix, etc)   \n",
        "- \n",
        "Training: The model is trained over a specified number of epochs. In each epoch, the model is set to training mode and the dataset is looped over using a DataLoader (train_loader). For each batch of data, the inputs and labels are moved to the GPU. The optimizer's gradients are zeroed out, and a forward pass is performed to get the model's outputs. The loss between the outputs and the labels is calculated using a classification loss function (cla_loss). The loss is then backpropagated through the network, and the optimizer performs a step to update the model's parameters. The accuracy of the model on the training data is calculated by comparing the model's predictions (obtained by taking the argmax of the outputs) with the true labels.\n",
        "\n",
        "Validation: After each epoch of training, the model is set to evaluation mode and validated on a validation set. The process is similar to the training loop, but no backpropagation or parameter updating is performed. The validation loss and accuracy are calculated and printed.\n",
        "\n",
        "Testing: After all epochs, the model is evaluated on a test set. The process is the same as the validation loop. The test loss and accuracy are calculated and printed.\n",
        "\n",
        "The performance of the model is assessed primarily through the loss and accuracy. The loss provides a measure of how well the model's predictions match the true labels, while the accuracy provides a measure of the proportion of inputs that the model correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TrainFunc2(model):\n",
        "    cla_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) \n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        model.train()\n",
        "        print(\"Epoch\", epoch+1, \"start training...\")\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels_smile = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs_smile = model(inputs)\n",
        "            loss_cla = cla_loss(outputs_smile, labels_smile.long())\n",
        "            loss_cla.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # 计算精度\n",
        "            preds_smile = torch.argmax(outputs_smile, dim=1)\n",
        "            acc_smile = torch.eq(preds_smile, labels_smile).sum().item() / labels_smile.size(0)\n",
        "            print(f'\\rEpoch {epoch+1}, Batch {i+1}, Loss(smile): {loss_cla.item():.4f}, Accuracy(smile): {acc_smile:.4f}',end='')\n",
        "        \n",
        "        scheduler.step()\n",
        "        # 在验证集上验证\n",
        "        model.eval()\n",
        "        val_loss_cla = 0\n",
        "        val_acc_smile = 0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader, 0):\n",
        "                inputs, labels_smile = data[0].to(device), data[1].to(device)\n",
        "                outputs_smile = model(inputs)\n",
        "                loss_cla = cla_loss(outputs_smile, labels_smile.long())\n",
        "                val_loss_cla += loss_cla.item()\n",
        "                preds_smile = torch.argmax(outputs_smile, dim=1)\n",
        "                val_acc_smile += accuracy_score(labels_smile.cpu(), preds_smile.cpu())\n",
        "        val_loss_cla /= len(val_loader)\n",
        "        val_acc_smile /= len(val_loader)\n",
        "        print(f'\\nEpoch {epoch+1}, Validation Loss(smile): {val_loss_cla:.4f}, Validation Accuracy(smile): {val_acc_smile:.4f}')\n",
        "\n",
        "    # 7. 评估模型\n",
        "    # 在测试集上测试\n",
        "    model.eval()\n",
        "    test_loss_cla = 0\n",
        "    test_acc_smile = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, labels_smile= data[0].to(device), data[1].to(device)\n",
        "            outputs_smile = model(inputs)\n",
        "            loss_cla = cla_loss(outputs_smile, labels_smile.long())\n",
        "            test_loss_cla += loss_cla.item()\n",
        "            preds_smile = torch.argmax(outputs_smile, dim=1)\n",
        "            test_acc_smile += accuracy_score(labels_smile.cpu(), preds_smile.cpu())\n",
        "    test_loss_cla /= len(test_loader)\n",
        "    test_acc_smile /= len(test_loader)\n",
        "    print(f'\\nTest Loss(smile): {test_loss_cla:.4f}, Test Accuracy(smile): {test_acc_smile:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "reg_loss = nn.MSELoss()\n",
        "def TrainFunc3(model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) \n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        model.train()\n",
        "        print(\"Epoch\", epoch+1, \"start training...\")\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels_pose = data[0].to(device), data[2].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs_pose = model(inputs)\n",
        "            # loss_reg = torch.nn.functional.mse_loss(outputs_pose, labels_pose.float())\n",
        "            loss_reg = reg_loss(outputs_pose, labels_pose.float())\n",
        "            loss_reg.backward()\n",
        "            optimizer.step()\n",
        "            mse_pose = mean_squared_error(labels_pose.cpu().numpy(), outputs_pose.detach().cpu().numpy())\n",
        "            print(f'\\rEpoch {epoch+1}, Batch {i+1}, MSE(pose): {mse_pose:.4f}',end='')\n",
        "        \n",
        "        scheduler.step()\n",
        "        # 在验证集上验证\n",
        "        model.eval()\n",
        "        val_loss_reg = 0\n",
        "        val_mse_pose = 0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader, 0):\n",
        "                inputs, labels_pose = data[0].to(device), data[2].to(device)\n",
        "                outputs_pose = model(inputs)\n",
        "                # loss_reg = torch.nn.functional.mse_loss(outputs_pose, labels_pose.float())\n",
        "                loss_reg = reg_loss(outputs_pose, labels_pose.float())\n",
        "                val_loss_reg += loss_reg.item()\n",
        "                val_mse_pose += mean_squared_error(labels_pose.cpu().numpy(), outputs_pose.detach().cpu().numpy())\n",
        "        val_loss_reg /= len(val_loader)\n",
        "        val_mse_pose /= len(val_loader)\n",
        "        print(f'\\nEpoch {epoch+1}, Validation MSE(pose): {val_mse_pose:.4f}')\n",
        "\n",
        "    # 7. 评估模型\n",
        "    # 在测试集上测试\n",
        "    model.eval()\n",
        "    test_loss_reg = 0\n",
        "    test_mse_pose = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, labels_pose= data[0].to(device), data[2].to(device)\n",
        "            outputs_pose = model(inputs)\n",
        "            # loss_reg = torch.nn.functional.mse_loss(outputs_pose, labels_pose.float())\n",
        "            loss_reg = reg_loss(outputs_pose, labels_pose.float())\n",
        "            test_loss_reg += loss_reg.item()\n",
        "            test_mse_pose += mean_squared_error(labels_pose.cpu().numpy(), outputs_pose.detach().cpu().numpy())\n",
        "    test_loss_reg /= len(test_loader)\n",
        "    test_mse_pose /= len(test_loader)\n",
        "    print(f'\\nTest MSE(pose): {test_mse_pose:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQPxztuL9AW"
      },
      "source": [
        "# 7 Dataset\n",
        "\n",
        "Describe the dataset that you will use to create your models and validate them. If you need to preprocess it, do it here. Include visualisations too. You can visualise raw data samples or extracted features.\n",
        "- \n",
        "The dataset used in this project is the genki4k dataset. It contains 4000 images of faces, with smiling faces and non-smiling faces. The dataset is divided into a training set, a validation set, and a test set, with 3000, 500, and 500 images respectively. The dataset is preprocessed by transforming all images into the same RGB format and size 224x224, introducing random horizontal flipping during training, and standardizing the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = './genki4k/'\n",
        "labels = []\n",
        "with open(os.path.join(dataset_path, 'labels.txt'), 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        items = line.split()\n",
        "        label = [int(items[0])] + [float(x) for x in items[1:]]  # 第一个是整数，后面三个是浮点数\n",
        "        labels.append(label)\n",
        "\n",
        "class Genki4kDataset(Dataset):\n",
        "    def __init__(self, img_dir, labels, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, 'files', f'file{idx+1:04}.jpg')  # 图片名称格式为file0001.jpg, file0002.jpg, ...\n",
        "        image = Image.open(img_name)\n",
        "        label_smile = self.labels[idx][0]\n",
        "        label_pose = torch.tensor([self.labels[idx][1], self.labels[idx][2], self.labels[idx][3]])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_smile, label_pose\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda image: image.convert('RGB')),  # 将所有图像转换为RGB图像\n",
        "    transforms.Resize((224, 224)),  # 将所有图像调整为224x224\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 对每个通道进行归一化  \n",
        "])\n",
        "\n",
        "dataset = Genki4kDataset(dataset_path, labels, transform)\n",
        "total_size = len(dataset)  # 数据集大小\n",
        "train_size = int(0.75 * total_size)  # 训练集\n",
        "val_size = int(0.125 * total_size)    # 验证集\n",
        "test_size = total_size - train_size - val_size  # 剩余部分为测试集\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qf7GN1aeXJI"
      },
      "source": [
        "# 8 Results\n",
        "\n",
        "Carry out your experiments here, explain your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model2...\n",
            "Epoch: 1 start training...\n",
            "Epoch 1, Batch 94, Loss(smile): 1.9177, Accuracy(smile): 0.3750\n",
            "Epoch 1, Validation Loss(smile): 0.8908, Validation Accuracy(smile): 0.6328\n",
            "Epoch: 2 start training...\n",
            "Epoch 2, Batch 94, Loss(smile): 0.8532, Accuracy(smile): 0.6667\n",
            "Epoch 2, Validation Loss(smile): 1.0150, Validation Accuracy(smile): 0.6602\n",
            "Epoch: 3 start training...\n",
            "Epoch 3, Batch 94, Loss(smile): 0.6484, Accuracy(smile): 0.6667\n",
            "Epoch 3, Validation Loss(smile): 0.9225, Validation Accuracy(smile): 0.6559\n",
            "Epoch: 4 start training...\n",
            "Epoch 4, Batch 94, Loss(smile): 1.5133, Accuracy(smile): 0.6250\n",
            "Epoch 4, Validation Loss(smile): 1.1140, Validation Accuracy(smile): 0.6480\n",
            "Epoch: 5 start training...\n",
            "Epoch 5, Batch 94, Loss(smile): 1.2884, Accuracy(smile): 0.5417\n",
            "Epoch 5, Validation Loss(smile): 1.1631, Validation Accuracy(smile): 0.6113\n",
            "Epoch: 6 start training...\n",
            "Epoch 6, Batch 94, Loss(smile): 0.9984, Accuracy(smile): 0.6667\n",
            "Epoch 6, Validation Loss(smile): 0.9705, Validation Accuracy(smile): 0.6773\n",
            "Epoch: 7 start training...\n",
            "Epoch 7, Batch 94, Loss(smile): 0.4027, Accuracy(smile): 0.7917\n",
            "Epoch 7, Validation Loss(smile): 0.9887, Validation Accuracy(smile): 0.6637\n",
            "Epoch: 8 start training...\n",
            "Epoch 8, Batch 94, Loss(smile): 0.5506, Accuracy(smile): 0.7500\n",
            "Epoch 8, Validation Loss(smile): 0.9990, Validation Accuracy(smile): 0.6402\n",
            "Epoch: 9 start training...\n",
            "Epoch 9, Batch 94, Loss(smile): 0.3909, Accuracy(smile): 0.8333\n",
            "Epoch 9, Validation Loss(smile): 0.8853, Validation Accuracy(smile): 0.6949\n",
            "Epoch: 10 start training...\n",
            "Epoch 10, Batch 94, Loss(smile): 0.7439, Accuracy(smile): 0.6250\n",
            "Epoch 10, Validation Loss(smile): 0.8818, Validation Accuracy(smile): 0.6656\n",
            "\n",
            "Test Loss(smile): 1.0205, Test Accuracy(smile): 0.6246\n"
          ]
        }
      ],
      "source": [
        "model2 = AlexNet2().to(device)\n",
        "print(\"Training model2...\")\n",
        "TrainFunc2(model2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\three\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\three\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "Training model3...\n",
            "Epoch 1 start training...\n",
            "Epoch 1, Batch 94, MSE(pose): 0.3644\n",
            "Epoch 1, Validation MSE(pose): 0.2625\n",
            "Epoch 2 start training...\n",
            "Epoch 2, Batch 94, MSE(pose): 0.6048\n",
            "Epoch 2, Validation MSE(pose): 0.6885\n",
            "Epoch 3 start training...\n",
            "Epoch 3, Batch 94, MSE(pose): 1.3643\n",
            "Epoch 3, Validation MSE(pose): 0.9089\n",
            "Epoch 4 start training...\n",
            "Epoch 4, Batch 94, MSE(pose): 1.8211\n",
            "Epoch 4, Validation MSE(pose): 1.1087\n",
            "Epoch 5 start training...\n",
            "Epoch 5, Batch 94, MSE(pose): 2.8323\n",
            "Epoch 5, Validation MSE(pose): 1.0578\n",
            "Epoch 6 start training...\n",
            "Epoch 6, Batch 94, MSE(pose): 1.5832\n",
            "Epoch 6, Validation MSE(pose): 0.5532\n",
            "Epoch 7 start training...\n",
            "Epoch 7, Batch 94, MSE(pose): 0.4120\n",
            "Epoch 7, Validation MSE(pose): 0.3473\n",
            "Epoch 8 start training...\n",
            "Epoch 8, Batch 94, MSE(pose): 0.8337\n",
            "Epoch 8, Validation MSE(pose): 0.2682\n",
            "Epoch 9 start training...\n",
            "Epoch 9, Batch 94, MSE(pose): 0.4791\n",
            "Epoch 9, Validation MSE(pose): 0.2468\n",
            "Epoch 10 start training...\n",
            "Epoch 10, Batch 94, MSE(pose): 0.3803\n",
            "Epoch 10, Validation MSE(pose): 0.2343\n",
            "\n",
            "Test MSE(pose): 0.2222\n"
          ]
        }
      ],
      "source": [
        "model3 = AlexNet3().to(device)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Training model3...\")\n",
        "TrainFunc3(model3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrJCR_cekPO"
      },
      "source": [
        "# 9 Conclusions\n",
        "\n",
        "Your conclusions, improvements, etc should go here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10 code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\three\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "tf.__version__"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
